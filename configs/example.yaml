server:
  host: 0.0.0.0
  port: 8000

  # Explicit GPU + memory-friendly defaults for Colab A100.
  # (Server code should load with vLLM on CUDA by default.)
  model: meta-llama/Llama-3.1-8B-Instruct
  dtype: float16

  # Generation params (kept constant for phase-transition experiments)
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.95
  seed: 42

  # Batching / scheduling knobs
  dynamic_batching: true
  max_num_seqs: 64
  max_num_batched_tokens: 4096
  scheduler_delay_ms: 5

loadgen:
  arrival_rate_rps: 12.5
  concurrency: 16
  duration_s: 120
  warmup_s: 15
  repeats: 3
  prompt_source: data/prompts_sample.jsonl
  mix:
    short: 0.5
    med: 0.3
    long: 0.2

eval:
  enable_embeddings: true
  embedding_model: sentence-transformers/all-MiniLM-L6-v2

experiment:
  name: baseline-balanced-mix
  output_dir: outputs/baseline-balanced-mix
